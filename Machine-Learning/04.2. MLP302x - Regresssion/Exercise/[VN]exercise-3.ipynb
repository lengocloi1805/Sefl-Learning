{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MdQmbilN1Az"
   },
   "source": [
    "# LASSO (coordinate descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nib8YtjHN1A1"
   },
   "source": [
    "Trong notebook này, chúng ta sẽ triển khai bộ giải LASSO qua coordinate descent. Chúng ta sẽ:\n",
    "* Viết một hàm chuẩn hóa các đặc trưng\n",
    "* Triển khai coordinate descent cho LASSO\n",
    "* Khám phán tác động của L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdvQZPG8N1A5"
   },
   "source": [
    "## Như thường lệ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "8zYugIAXN1A7"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQwqSfliN1A-"
   },
   "source": [
    "## Load dữ liệu doanh số bán nhà\n",
    "\n",
    "Tập dữ liệu doanh số bán nhà ở quận King, Seatle, WA. Nghe quen chứ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "9HSohLu5N1A_"
   },
   "outputs": [],
   "source": [
    "full_data = pandas.read_csv(\"kc_house_data.csv\", index_col=0)\n",
    "# Trong tập dữ liệu, 'floors' được xác định là kiểu string,\n",
    "# nên chúng ta sẽ chuyển đổi chúng thành int trước khi sử dụng dưới đây\n",
    "full_data['floors'] = full_data['floors'].astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BP175k_YN1BA"
   },
   "source": [
    "Nếu muốn thực hiện bất kỳ \"feature engineering\" nào như tạo các đặc trưng mới hoặc điều chỉnh đặc trưng sẵn có, chúng ta có thể sửa DataFrame của pandas như trong lab trước (Lab 2). Tuy nhiên, với notebook này, chúng ta sẽ làm việc với các đặc trưng có sẵn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1b_KaNoN1BB"
   },
   "source": [
    "## Import các hàm hữu ích từ notebook trước"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VV9sdXSN1BC"
   },
   "source": [
    "Như trong Exercise 1, chúng ta chuyển đổi DataFrame thành ma trận Numpy 2D. Copy và paste `get_numpy_data()` từ exercise đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "cyzAEMJVN1BD"
   },
   "outputs": [],
   "source": [
    "import numpy as np # điều này cho phép gọi numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "sgPkydiGN1BD"
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(data, features_title, output_title):\n",
    "    if('constant' not in data):\n",
    "        data['constant'] = 1 # đây là cách thêm cột constant. Chỉ thực hiện khi cần \n",
    "    # thêm cột 'constant' vào trước list các đặc trưng để chúng ta có thể trích xuất cùng với những thứ khác:\n",
    "    features_title = ['constant'] + features_title # đây là cách kết hợp 2 list\n",
    "    # chia dữ liệu thành sub-DataFrame chứa các đặc trưng đã chỉ định (gồm constant)\n",
    "    # gọi nó là features_columns.\n",
    "    features_columns = data[features_title]\n",
    "    # dòng tiếp theo sẽ trích xuất ma trận numpy từ biến features_columns:\n",
    "    feature_matrix = features_columns.values\n",
    "    # truy xuất dữ liệu được liên kết với đầu ra trong pandas Series\n",
    "    # gọi nó là output_column\n",
    "    output_column = data[output_title]\n",
    "    # tiếp theo sẽ chuyển đổi Series đã nhắc thành một mảng numpy\n",
    "    output_array = output_column.values\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pznB8L7mN1BE"
   },
   "source": [
    "Cũng copy và paste cả hàm `predict_output()` để tính các dự đoán cho toàn bộ ma trận đặc trưng với ma trận và trọng số đã cho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "OlEq3F0fN1BF"
   },
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    # giả sử ma trận feature_matrix chứa các đặc trưng ở dạng các cột và trọng số là mảng numpy tương ứng\n",
    "    # tạo vectơ dự đoán sử dụng np.dot()\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOhG_MW8N1BF"
   },
   "source": [
    "## Chuẩn hóa các đặc trưng\n",
    "Trong tập dữ liệu giá nhà, các đặc trưng thay đổi khá nhiều về độ lớn: ví dụ, `sqft_living` rất lớn so với `bedrooms`. Do đó, trọng số cho `sqft_living` sẽ nhỏ hơn rất nhiều so với trọng số cho `bedrooms`. Điều này khó giải quyết vì các trọng số \"nhỏ\" bị giảm đầu tiên khi `l1_penalty` tăng. \n",
    "\n",
    "Để công bằng với tất cả các đặc trưng, chúng ta cần **chuẩn hóa đặc trưng** như đã thảo luận trong các bài giảng: chia mỗi đặc trưng cho chuẩn 2 của nó để đặc trưng đã biến đổi có chuẩn 1.\n",
    "\n",
    "Hãy xem chúng ta có thể thực hiện chuẩn hóa này dễ dàng thế nào với Numpy: trước tiên chúng ta sẽ xem xét một ma trận nhỏ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "9TkaelZlN1BG",
    "outputId": "30d1c5e2-0d8e-4237-bbe1-136af59b74f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  5.  8.]\n",
      " [ 4. 12. 15.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[3.,5.,8.],[4.,12.,15.]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmnDlMT6N1BH"
   },
   "source": [
    "Numpy cung cấp cách viết tắt để tính toán chuẩn 2 của mỗi cột:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "i0g3cgp_N1BI",
    "outputId": "217bb9ab-b5f4-41f0-b21b-2e5fe8a1927a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5. 13. 17.]\n"
     ]
    }
   ],
   "source": [
    "norms = np.linalg.norm(X, axis=0) # cho [norm(X[:,0]), norm(X[:,1]), norm(X[:,2])]\n",
    "print(norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCZfkzGmN1BI"
   },
   "source": [
    "Để chuẩn hóa, hãy áp dụng phép chia element-wise (thực hiện trên các phần tử tương ứng):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "u22vfuSVN1BJ",
    "outputId": "f44110a2-62f9-49a1-dba3-4ea726061faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6        0.38461538 0.47058824]\n",
      " [0.8        0.92307692 0.88235294]]\n"
     ]
    }
   ],
   "source": [
    "print(X / norms) # cho [X[:,0]/norm(X[:,0]), X[:,1]/norm(X[:,1]), X[:,2]/norm(X[:,2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB8xsWmQN1BK"
   },
   "source": [
    "Với viết tắt mà chúng ta vừa đề cập, hãy viết một hàm ngắn là `normalize_features(feature_matrix)`, hàm này chuẩn hóa các cột của ma trận đặc trưng đã cho. Hàm phải sẽ về một cặp `(normalized_features, norms)`, trong đó mục thứ hai chứa chuẩn của các đặc trưng ban đầu. Như đã thảo luận trong các bài giảng, chúng ta sẽ sử dụng các chuẩn này để chuẩn hóa dữ liệu kiểm tra theo cách mà chúng ta chuẩn hóa dữ liệu huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "q5dH637MN1BL"
   },
   "outputs": [],
   "source": [
    "def normalize_features(feature_matrix):\n",
    "    norms = np.linalg.norm(feature_matrix, axis=0)\n",
    "    features = feature_matrix/norms\n",
    "    return features, norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRyCTlqXN1BM"
   },
   "source": [
    "Để kiểm tra đạo hàm, chạy cell sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6 0.6 0.6]\n",
      " [0.8 0.8 0.8]]\n",
      "[ 5. 10. 15.]\n"
     ]
    }
   ],
   "source": [
    "features, norms = normalize_features(np.array([[3.,6.,9.],[4.,8.,12.]]))\n",
    "print(features)\n",
    "# sẽ in ra\n",
    "# [[ 0.6  0.6  0.6]\n",
    "#  [ 0.8  0.8  0.8]]\n",
    "print(norms)\n",
    "# sẽ in ra\n",
    "# [5.  10.  15.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v33OSKiaN1BO"
   },
   "source": [
    "## Triển khai Coordinate Descent với các đặc trưng được chuẩn hóa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vda1_GtEN1BO"
   },
   "source": [
    "Chúng ta tìm cách thu được một tập hợp trọng số thưa thớt bằng cách giảm thiểu hàm chi phí LASSO\n",
    "<!-- ``` SUM[ (prediction - output)^2 ] + lambda*( |w[1]| + ... + |w[k]|). ``` -->\n",
    "$cost = \\sum (prediction - output)^2 + \\lambda * \\sum_{i \\neq 0} |w_i|$\n",
    "\n",
    "(Theo quy ước, chúng ta không bao hàm $w_0$ (độ chệch) trong phần tử L1 penalty. Chúng ta không bao giờ đẩy intercept thành 0.)\n",
    "\n",
    "Dấu giá trị tuyệt đối làm cho hàm chi phí không thể phân biệt được, do đó gradient descent đơn giản không khả thi (bạn sẽ cần triển khai phương pháp subgradient descent). Thay vào đó, chúng ta sẽ sử dụng **coordinate descent**: ở mỗi lần lặp, chúng ta sẽ cố định tất cả các trọng số ngoại trừ trọng số `i` và tìm giá trị trọng số` i` thu nhỏ mục tiêu, tức là tìm: <br>\n",
    "<!-- ``` argmin_{w[i]} [ SUM[ (prediction - output)^2 ] + lambda*( |w[1]| + ... + |w[k]|) ] ``` -->\n",
    "$argmin_{w_i}(cost_i) = \\sum (prediction - output)^2 + \\lambda * \\sum_{i \\neq 0} |w_i| )$\n",
    "\n",
    "trong đó tất cả các trọng số khác $w_i$(`w[i]`) được coi là không đổi. Chúng ta sẽ tối ưu hóa $w_i$ lần lượt, tuần hoàn nhiều lần qua các trọng số.\n",
    "  1. Chọn tọa độ `i`\n",
    "  2. Tính $w_i$ giảm thiểu hàm chi phí $cost = (\\sum prediction - output) + \\lambda * \\sum_{i \\neq 0} |w_i|$\n",
    "  3. Lặp lại bước 1 và 2 cho tất cả các tọa độ nhiều lần. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnjPH4xiN1BP"
   },
   "source": [
    "Với notebook này, chúng ta sử dụng **coordinate descent theo chu kỳ với các đặc trưng được chuẩn hóa**, trong đó chúng ta tuần hoàn qua các tọa độ theo thứ tự từ 0 đến (d-1) và giả sử các đặc trưng đã được chuẩn hóa như đã thảo luận ở trên. Công thức để tối ưu hóa từng tọa độ như sau:\n",
    "<!-- ```\n",
    "       ┌ (ro[i] + lambda/2)     if ro[i] < -lambda/2\n",
    "w[i] = ├ 0                      if -lambda/2 <= ro[i] <= lambda/2\n",
    "       └ (ro[i] - lambda/2)     if ro[i] > lambda/2\n",
    "``` -->\n",
    "$w_i = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\rho_i + \\lambda / 2 & \\rho_i < -\\lambda/2 \\\\\n",
    "      0 & -\\lambda/2 \\leq \\rho_i \\leq \\lambda/2  \\\\\n",
    "      \\rho_i - \\lambda / 2 & \\rho_i > \\lambda/2 \\\\\n",
    "\\end{array} \n",
    "\\right. $\n",
    "\n",
    "trong đó $\\rho_i$(`ro[i]`) được xác định như sau:\n",
    "<!-- ```ro[i] = SUM[ [feature_i]*(output - prediction + w[i]*[feature_i]) ]. ``` -->\n",
    "$\\rho_i = \\sum feature_i * (output - prediction + w_i*feature_i)$\n",
    "\n",
    "Lưu ý là chúng ta không điều chuẩn trọng số của đặc trưng không đổi (intercept|độ chệch) $w_0$(`w[0]`), nên với trọng số này cập nhật đơn giản là:\n",
    "<!-- ```w[0] = ro[i]``` -->\n",
    "$w_0 = \\rho_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skAti6TvN1BS"
   },
   "source": [
    "## Tác động của L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTl8kwiVN1BT"
   },
   "source": [
    "Xét mô hình đơn giản có 2 đặc trưng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "8KAkrEDnN1BT"
   },
   "outputs": [],
   "source": [
    "simple_features = ['sqft_living', 'bedrooms']\n",
    "my_output = 'price'\n",
    "(simple_feature_matrix, output) = get_numpy_data(full_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmpyXA83N1BU"
   },
   "source": [
    "Đừng quên chuẩn hóa các đặc trưng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "FJdk7zgrN1BU"
   },
   "outputs": [],
   "source": [
    "simple_feature_matrix, norms = normalize_features(simple_feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozwVirpIN1BV"
   },
   "source": [
    "Chúng ta gán một số tập hợp các trọng số ban đầu ngẫu nhiên và kiểm tra các giá trị của `ro[i]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "GTwEXPYGN1BV"
   },
   "outputs": [],
   "source": [
    "weights = np.array([1., 4., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15PVSOZKN1BV"
   },
   "source": [
    "Sử dụng `predict_output()` để đưa ra dự đoán trên dữ liệu này. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "LuKaVGw-N1BW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02675867, 0.04339256, 0.01990703, ..., 0.02289873, 0.03178473,\n",
       "       0.02289873])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict_output(simple_feature_matrix, weights)# easy\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w4npMcbN1BW"
   },
   "source": [
    "Tính giá trị của `ro[i]` cho từng đặc trưng trong mô hình đơn giản này sử dụng công thức đã cho:\n",
    "<!-- ```ro[i] = SUM[ [feature_i]*(output - prediction + w[i]*[feature_i]) ]. ``` -->\n",
    "$\\rho_i = \\sum feature_i * (output - prediction + w_i*feature_i)$\n",
    "\n",
    "*Gợi ý: có thể sử dụng vectơ Numpy cho feature_i bằng:*\n",
    "```\n",
    "simple_feature_matrix[:,i]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "89pwp0rHN1BX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ro values: [79400300.0145229, 87939470.82325175, 80966698.66623947]\n"
     ]
    }
   ],
   "source": [
    "# Nếu còn băn khoăn, hãy áp dụng trực tiếp công thức trên. \n",
    "ro = []\n",
    "for i in range(len(weights)):\n",
    "    ro_i = np.sum(simple_feature_matrix[:, i]*(output - prediction + weights[i]*simple_feature_matrix[:, i]))\n",
    "    ro.append(ro_i)\n",
    "print(f'ro values: {ro}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3mTWA6lN1BX"
   },
   "source": [
    "***QUIZ***\n",
    "\n",
    "Nhớ lại rằng, bất cứ khi nào `ro[i]` nằm trong khoảng `-l1_penalty/2` và `l1_penalty/2` thì trọng số `w[i]` tương ứng sẽ về 0. Bây giờ, giả sử chúng ta thực hiện một bước coordinate descent ở đặc trưng 1 hoặc đặc trưng 2. Phạm vi giá trị nào của `l1_penalty`sẽ **không đặt** `w[1]` thành 0 mà **đặt** `w[2]` thành 0 nếu chúng ta lấy một bước trong tọa độ đó?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "uddncnUHN1BY"
   },
   "outputs": [],
   "source": [
    "# Return True if value is within the threshold ranges otherwise False\n",
    "# Looking for range -l1_penalty/2 <= ro <= l1_penalty/2\n",
    "def in_l1range(value, penalty):\n",
    "    return (value >= -penalty/2.) and (value <= penalty/2.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHorg9d4N1BY"
   },
   "source": [
    "***QUIZ***\n",
    "\n",
    "Phạm vi giá trị nào của `l1_penalty` sẽ đặt **cả** `w[1]` và `w[2]` thành 0 nếu lấy một bước trong tọa độ đó?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svXX4GntN1BZ"
   },
   "source": [
    "Có thể nói rằng `ro[i]` định lượng tầm quan trọng của đặc trưng thứ i: `ro[i]` càng lớn thì đặc trưng thứ i càng có khả năng được giữ lại."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ti4WBIqN1BZ"
   },
   "source": [
    "## Single Coordinate Descent Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_ptlpgxN1Ba"
   },
   "source": [
    "Using the formula above, implement coordinate descent that minimizes the cost function over a single feature i. Note that the intercept (weight 0) is not regularized. The function should accept feature matrix, output, current weights, l1 penalty, and index of feature to optimize over. The function should return new weight for feature i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "U2TQaqzNN1Ba"
   },
   "outputs": [],
   "source": [
    "def lasso_coordinate_descent_step(i, feature_matrix, output, weights, l1_penalty):\n",
    "    # compute prediction\n",
    "    prediction = predict_output(feature_matrix, weights)\n",
    "    # compute ro[i] = SUM[ [feature_i]*(output - prediction + weight[i]*[feature_i]) ]\n",
    "    ro_i = np.sum(feature_matrix[:, i]*(output - prediction + weights[i]*feature_matrix[:, i]))\n",
    "\n",
    "    if i == 0: # intercept -- do not regularize\n",
    "        new_weight_i = ro_i \n",
    "    elif ro_i < -l1_penalty/2.:\n",
    "        new_weight_i = ro_i + l1_penalty/2\n",
    "    elif ro_i > l1_penalty/2.:\n",
    "        new_weight_i = ro_i - l1_penalty/2\n",
    "    else:\n",
    "        new_weight_i = 0.\n",
    "    \n",
    "    return new_weight_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOnn2nCON1Bb"
   },
   "source": [
    "To test the function, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4255588466910251\n"
     ]
    }
   ],
   "source": [
    "# should print 0.425558846691\n",
    "import math\n",
    "print(lasso_coordinate_descent_step(1, np.array([[3./math.sqrt(13),1./math.sqrt(10)],[2./math.sqrt(13),3./math.sqrt(10)]]), \n",
    "                                   np.array([1., 1.]), np.array([1., 4.]), 0.1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "ZypEFnXhN1Bc"
   },
   "source": [
    "## Cyclical coordinate descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBG7nRE0N1Bc"
   },
   "source": [
    "Now that we have a function that optimizes the cost function over a single coordinate, let us implement cyclical coordinate descent where we optimize coordinates 0, 1, ..., (d-1) in order and repeat.\n",
    "\n",
    "When do we know to stop? Each time we scan all the coordinates (features) once, we measure the change in weight for each coordinate. If no coordinate changes by more than a specified threshold, we stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuzrhWz8N1Bd"
   },
   "source": [
    "For each iteration:\n",
    "1. As you loop over features in order and perform coordinate descent, measure how much each coordinate changes.\n",
    "2. After the loop, if the maximum change across all coordinates is falls below the tolerance, stop. Otherwise, go back to step 1.\n",
    "\n",
    "Return weights\n",
    "\n",
    "**IMPORTANT: when computing a new weight for coordinate i, make sure to incorporate the new weights for coordinates 0, 1, ..., i-1. One good way is to update your weights variable in-place. See following pseudocode for illustration.**\n",
    "```\n",
    "for i in range(len(weights)):\n",
    "    old_weights_i = weights[i] # remember old value of weight[i], as it will be overwritten\n",
    "    # the following line uses new values for weight[0], weight[1], ..., weight[i-1]\n",
    "    #     and old values for weight[i], ..., weight[d-1]\n",
    "    weights[i] = lasso_coordinate_descent_step(i, feature_matrix, output, weights, l1_penalty)\n",
    "    \n",
    "    # use old_weights_i to compute change in coordinate\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "JWCzx6lxN1Bf"
   },
   "outputs": [],
   "source": [
    "def lasso_cyclical_coordinate_descent(feature_matrix, output, initial_weights, l1_penalty, tolerance):\n",
    "    D = feature_matrix.shape[1]\n",
    "    weights = np.array(initial_weights)\n",
    "    \n",
    "    change = np.array(initial_weights) * 0.0\n",
    "    converged = False\n",
    "    \n",
    "    while not converged:\n",
    "        for i in range(D):\n",
    "            old_weight = weights[i]\n",
    "            weights[i] = lasso_coordinate_descent_step(i, feature_matrix, output, weights, l1_penalty)\n",
    "\n",
    "            change[i] = np.abs(weights[i] - old_weight)\n",
    "\n",
    "        if max(change) < tolerance:\n",
    "            converged = True\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8ykbh6rN1Bg"
   },
   "source": [
    "Using the following parameters, learn the weights on the sales dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "BMviCcFFN1Bh"
   },
   "outputs": [],
   "source": [
    "simple_features = ['sqft_living', 'bedrooms']\n",
    "my_output = 'price'\n",
    "initial_weights = np.zeros(3)\n",
    "l1_penalty = 1e7\n",
    "tolerance = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGxqt_dWN1Bi"
   },
   "source": [
    "First create a normalized version of the feature matrix, `normalized_simple_feature_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "1Z6c6lZbN1Bj"
   },
   "outputs": [],
   "source": [
    "(simple_feature_matrix, output) = get_numpy_data(full_data, simple_features, my_output)\n",
    "(normalized_simple_feature_matrix, simple_norms) = normalize_features(simple_feature_matrix) # normalize features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTMAvSB0N1Bk"
   },
   "source": [
    "Then, run your implementation of LASSO coordinate descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "PXYzoC5GN1Bk",
    "outputId": "bb07db2f-9b0f-4693-8816-45380b23854c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.63E+15\n",
      "['constant' 'sqft_living']\n"
     ]
    }
   ],
   "source": [
    "weights = lasso_cyclical_coordinate_descent(normalized_simple_feature_matrix, output,\n",
    "                                            initial_weights, l1_penalty, tolerance)\n",
    "prediction = predict_output(normalized_simple_feature_matrix, weights)\n",
    "rss = np.sum((output - prediction) ** 2)\n",
    "print(\"{:.2E}\".format(rss))\n",
    "print(np.array(['constant'] + simple_features)[weights != 0])\n",
    "# Result should be:\n",
    "# # 1.63E+15\n",
    "# # ['constant' 'sqft_living']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmJ6EOh1N1Bl"
   },
   "source": [
    "***QUIZ QUESTIONS***\n",
    "1. What is the RSS of the learned model on the normalized dataset? (Hint: use the normalized feature matrix when you make predictions.)\n",
    "2. Which features had weight zero at convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaBiktozN1Bl"
   },
   "source": [
    "# Evaluating LASSO fit with more features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Inyx3rdQN1Bm"
   },
   "source": [
    "Let us split the sales dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "YgOUVExKN1Bm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(full_data, train_size=0.8, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c6H7iDLN1Bn"
   },
   "source": [
    "Let us consider the following set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "mRPzri1UN1Bn"
   },
   "outputs": [],
   "source": [
    "all_features = ['bedrooms',\n",
    "                'bathrooms',\n",
    "                'sqft_living',\n",
    "                'sqft_lot',\n",
    "                'floors',\n",
    "                'waterfront', \n",
    "                'view', \n",
    "                'condition', \n",
    "                'grade',\n",
    "                'sqft_above',\n",
    "                'sqft_basement',\n",
    "                'yr_built', \n",
    "                'yr_renovated']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D02hfdGQN1Bo"
   },
   "source": [
    "First, create a normalized feature matrix from the TRAINING data with these features.  (Make you store the norms for the normalization, since we'll use them later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "-To0uJtiN1Bo"
   },
   "outputs": [],
   "source": [
    "# use normalize_features, goldfish.\n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, all_features, my_output)\n",
    "normalized_feature_matrix, norms = normalize_features(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXTg5ux4N1Bp"
   },
   "source": [
    "First, learn the weights with `l1_penalty=1e7`, on the training data. Initialize weights to all zeros, and set the `tolerance=1`.  Call resulting weights `weights1e7`, you will need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "zIW5Z3WzN1Bp"
   },
   "outputs": [],
   "source": [
    "# Handholding level: nonexistent. Not that you will need any at this point.\n",
    "initial_weights = np.zeros(len(all_features) + 1)\n",
    "l1_penalty = 1e7\n",
    "tolerance = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24625317.01587637,        0.        ,        0.        ,\n",
       "       48292264.53592271,        0.        ,        0.        ,\n",
       "        3566000.33284847,  7734349.09716174,        0.        ,\n",
       "              0.        ,        0.        ,        0.        ,\n",
       "              0.        ,        0.        ])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights1e7 = lasso_cyclical_coordinate_descent(normalized_feature_matrix, output,\n",
    "                                               initial_weights, l1_penalty, tolerance)\n",
    "weights1e7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWG_iyReN1Br"
   },
   "source": [
    "***QUIZ QUESTION***\n",
    "\n",
    "What features had non-zero weight in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constant', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above', 'sqft_basement', 'yr_built', 'yr_renovated']\n"
     ]
    }
   ],
   "source": [
    "feature_list = ['constant'] + all_features\n",
    "print(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 24625317.015876368\n",
      "sqft_living 48292264.535922706\n",
      "waterfront 3566000.3328484744\n",
      "view 7734349.097161738\n"
     ]
    }
   ],
   "source": [
    "feature_weights1e7 = dict(zip(feature_list, weights1e7))\n",
    "for k,v in feature_weights1e7.items():\n",
    "    if v != 0.0:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "FoMM_S49N1Br"
   },
   "source": [
    "Next, learn the weights with `l1_penalty=1e8`, on the training data. Initialize weights to all zeros, and set the `tolerance=1`.  Call resulting weights `weights1e8`, you will need them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "2_3NMvVeN1Br"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([71373534.79051217,        0.        ,        0.        ,\n",
       "              0.        ,        0.        ,        0.        ,\n",
       "              0.        ,        0.        ,        0.        ,\n",
       "              0.        ,        0.        ,        0.        ,\n",
       "              0.        ,        0.        ])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just change the number and try again.\n",
    "l1_penalty=1e8\n",
    "tolerance = 1.0\n",
    "weights1e8 = lasso_cyclical_coordinate_descent(normalized_feature_matrix, output,\n",
    "                                               initial_weights, l1_penalty, tolerance)\n",
    "weights1e8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nml3-n9oN1Bs"
   },
   "source": [
    "***QUIZ QUESTION***\n",
    "\n",
    "What features had non-zero weight in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 71373534.79051217\n"
     ]
    }
   ],
   "source": [
    "feature_weights1e8 = dict(zip(feature_list, weights1e8))\n",
    "for k,v in feature_weights1e8.items():\n",
    "    if v != 0.0:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa0PSzafN1Bs"
   },
   "source": [
    "Finally, learn the weights with `l1_penalty=1e4`, on the training data. Initialize weights to all zeros, and set the `tolerance=5e5`.  Call resulting weights `weights1e4`, you will need them later.  (This case will take [quite a bit longer](https://xkcd.com/303/) to converge than the others above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "YpXqYDw7N1Bt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 77664061.64562126, -19472165.15076051,  14984932.17111344,\n",
       "        88451858.26250325,  -2367122.75899844,  -7580888.92952496,\n",
       "         6722334.51845239,   7489670.90064896,   3978526.55485284,\n",
       "        13776609.14004955, -11474194.80073022,  -4874702.39717918,\n",
       "       -82543644.30150893,   2943815.78795278])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is closer to your future than you might think. Unless you fail the course, but hey.\n",
    "l1_penalty=1e4\n",
    "tolerance=5e5\n",
    "weights1e4 = lasso_cyclical_coordinate_descent(normalized_feature_matrix, output,\n",
    "                                               initial_weights, l1_penalty, tolerance)\n",
    "weights1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsYGeWJDN1Bt"
   },
   "source": [
    "***QUIZ QUESTION***\n",
    "\n",
    "What features had non-zero weight in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constant 77664061.64562126\n",
      "bedrooms -19472165.15076051\n",
      "bathrooms 14984932.171113444\n",
      "sqft_living 88451858.26250325\n",
      "sqft_lot -2367122.7589984355\n",
      "floors -7580888.92952496\n",
      "waterfront 6722334.518452395\n",
      "view 7489670.900648957\n",
      "condition 3978526.5548528424\n",
      "grade 13776609.14004955\n",
      "sqft_above -11474194.800730217\n",
      "sqft_basement -4874702.397179184\n",
      "yr_built -82543644.30150893\n",
      "yr_renovated 2943815.7879527835\n"
     ]
    }
   ],
   "source": [
    "feature_weights1e4 = dict(zip(feature_list, weights1e4))\n",
    "for k,v in feature_weights1e4.items():\n",
    "    if v != 0.0:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXinzirsN1Bt"
   },
   "source": [
    "## Rescaling learned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJgeR200N1Bu"
   },
   "source": [
    "Recall that we normalized our feature matrix, before learning the weights.  To use these weights on a test set, we must normalize the test data in the same way.\n",
    "\n",
    "Alternatively, we can rescale the learned weights to include the normalization, so we never have to worry about normalizing the test data: \n",
    "\n",
    "In this case, we must scale the resulting weights so that we can make predictions with *original* features:\n",
    " 1. Store the norms of the original features to a vector called `norms`:\n",
    "```\n",
    "features, norms = normalize_features(features)\n",
    "```\n",
    " 2. Run Lasso on the normalized features and obtain a `weights` vector\n",
    " 3. Compute the weights for the original features by performing element-wise division, i.e.\n",
    "```\n",
    "weights_normalized = weights / norms\n",
    "```\n",
    "Now, we can apply `weights_normalized` to the test data, without normalizing it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgUN_lAcN1Bv"
   },
   "source": [
    "Create a normalized version of each of the weights learned above. (`weights1e4`, `weights1e7`, `weights1e8`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "hWFYjyXsN1Bw"
   },
   "outputs": [],
   "source": [
    "# You remembered to store the weights, didn't you?\n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, all_features, my_output)\n",
    "normalized_feature_matrix, norms = normalize_features(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161.2952\n"
     ]
    }
   ],
   "source": [
    "normalized_weights1e7 = weights1e7 / norms\n",
    "normalized_weights1e8 = weights1e8 / norms\n",
    "normalized_weights1e4 = weights1e4 / norms\n",
    "print(\"{:.4f}\".format(normalized_weights1e7[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3y3mgPNN1Bw"
   },
   "source": [
    "To check your results, if you call `normalized_weights1e7` the normalized version of `weights1e7`, then:\n",
    "```\n",
    "print(\"{:.4f}\".format(normalized_weights1e7[3]))\n",
    "```\n",
    "should return 161.2952"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbKDd653N1Bx"
   },
   "source": [
    "## Evaluating each of the learned models on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6SYsB7EN1B0"
   },
   "source": [
    "Let's now evaluate the three models on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "9vPQvggCN1B0"
   },
   "outputs": [],
   "source": [
    "(test_feature_matrix, test_output) = get_numpy_data(test_data, all_features, 'price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4VgvjCrN1B0"
   },
   "source": [
    "Compute the RSS of each of the three normalized weights on the (unnormalized) `test_feature_matrix`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "n6rKuSmXN1B1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS for model with weights1e7 =  271730500343684.94\n"
     ]
    }
   ],
   "source": [
    "# If you need more cells, Insert -> Insert Cell Below.\n",
    "prediction =  predict_output(test_feature_matrix, normalized_weights1e7)\n",
    "RSS = np.dot(test_output-prediction, test_output-prediction)\n",
    "print('RSS for model with weights1e7 = ', RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS for model with weights1e8 =  514904176489231.25\n"
     ]
    }
   ],
   "source": [
    "# If you need more cells, Insert -> Insert Cell Below.\n",
    "prediction =  predict_output(test_feature_matrix, normalized_weights1e8)\n",
    "RSS = np.dot(test_output-prediction, test_output-prediction)\n",
    "print('RSS for model with weights1e8 = ', RSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS for model with weights1e4 =  225312368588044.56\n"
     ]
    }
   ],
   "source": [
    "prediction =  predict_output(test_feature_matrix, normalized_weights1e4)\n",
    "RSS = np.dot(test_output-prediction, test_output-prediction)\n",
    "print('RSS for model with weights1e4 = ', RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hzrk0bTDN1B1"
   },
   "source": [
    "***QUIZ QUESTION***\n",
    "\n",
    "Which model performed best on the test data?<br>\n",
    "==>model with weights1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================END==========================="
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[VN]exercise-3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
